{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate videos with different moving directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import json\n",
    "import os\n",
    "\n",
    "width, height = 400, 400\n",
    "motion_directions = [\"assets/motion_direction/direction_left.json\",\n",
    "                     \"assets/motion_direction/direction_right.json\",\n",
    "                     \"assets/motion_direction/direction_up.json\",\n",
    "                     \"assets/motion_direction/direction_down.json\",\n",
    "                     \"assets/motion_direction/direction_still.json\"]\n",
    "scene_directions = [\"assets/images/scene1.jpg\",\n",
    "                     \"assets/images/scene2.jpg\"]\n",
    "output_video_path = \"output/videos\"\n",
    "os.makedirs(output_video_path, exist_ok=True)\n",
    "\n",
    "for motion_direction in motion_directions:\n",
    "    with open(motion_direction) as f:\n",
    "        velocity_list = json.load(f)['direction']\n",
    "\n",
    "    for scene_direction in scene_directions:\n",
    "        x = 0\n",
    "        y = 0\n",
    "        color_numpy_array = []\n",
    "        for idx, velocity in enumerate(velocity_list):\n",
    "            output_path = os.path.join(output_video_path, motion_direction.split(\"/\")[-1].split(\".\")[0]+\"_\"+scene_direction.split(\"/\")[-1].split(\".\")[0]+\".gif\")\n",
    "            image = Image.open(scene_direction)\n",
    "            image_width, image_height = image.size\n",
    "            array = np.array(image)\n",
    "            x += velocity[0]\n",
    "            y += velocity[1]\n",
    "            frame = array[y+image_height//2:y+image_height//2+height,x+image_width//2:x+image_width//2+width]\n",
    "            color_numpy_array.append(frame)\n",
    "\n",
    "        modified_frames = [frame + np.random.randint(0, 2, frame.shape, dtype=np.uint8) for frame in color_numpy_array] # prevent saving optimization\n",
    "        imageio.mimsave(output_path, modified_frames, duration=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get intermediate feature from DDIM inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import do_inversion, load_pipeline\n",
    "import os\n",
    "\n",
    "video_path = [\"output/videos/direction_down_scene1.gif\",\n",
    "              \"output/videos/direction_down_scene2.gif\",\n",
    "              \"output/videos/direction_up_scene1.gif\",\n",
    "              \"output/videos/direction_up_scene2.gif\",\n",
    "              \"output/videos/direction_right_scene1.gif\",\n",
    "              \"output/videos/direction_right_scene2.gif\",\n",
    "              \"output/videos/direction_left_scene1.gif\",\n",
    "              \"output/videos/direction_left_scene2.gif\",\n",
    "              \"output/videos/direction_still_scene1.gif\",\n",
    "              \"output/videos/direction_still_scene2.gif\",]\n",
    "outpath = [\"output/inter_feat/direction_down_scene1.pt\",\n",
    "           \"output/inter_feat/direction_down_scene2.pt\",\n",
    "           \"output/inter_feat/direction_up_scene1.pt\",\n",
    "           \"output/inter_feat/direction_up_scene2.pt\",\n",
    "           \"output/inter_feat/direction_right_scene1.pt\",\n",
    "           \"output/inter_feat/direction_right_scene2.pt\",\n",
    "           \"output/inter_feat/direction_left_scene1.pt\",\n",
    "           \"output/inter_feat/direction_left_scene2.pt\",\n",
    "           \"output/inter_feat/direction_still_scene1.pt\",\n",
    "           \"output/inter_feat/direction_still_scene2.pt\",]\n",
    "\n",
    "os.makedirs('output/inter_feat', exist_ok=True)\n",
    "\n",
    "pipeline = load_pipeline()\n",
    "do_inversion(pipeline, video_path, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Content Correlation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_list = [\n",
    "    \"output/inter_feat/direction_down_scene1.pt\",\n",
    "    \"output/inter_feat/direction_down_scene2.pt\",\n",
    "    \"output/inter_feat/direction_up_scene1.pt\",\n",
    "    \"output/inter_feat/direction_up_scene2.pt\",\n",
    "    \"output/inter_feat/direction_right_scene1.pt\",\n",
    "    \"output/inter_feat/direction_right_scene2.pt\",\n",
    "    \"output/inter_feat/direction_left_scene1.pt\",\n",
    "    \"output/inter_feat/direction_left_scene2.pt\",\n",
    "    \"output/inter_feat/direction_still_scene1.pt\",\n",
    "    \"output/inter_feat/direction_still_scene2.pt\",\n",
    "]\n",
    "\n",
    "total_data_num = len(data_list)\n",
    "\n",
    "frame_num = 16\n",
    "\n",
    "data_tensor_origin = None\n",
    "for i, path in enumerate(data_list):\n",
    "    data = torch.load(path)\n",
    "    data = F.interpolate(data, [64, 64], mode='bilinear').to(torch.float32)\n",
    "\n",
    "    b,c,h,w = data.shape\n",
    "    crop_size = 16\n",
    "    data = data[:,:,h//2-crop_size//2:h//2+crop_size//2,w//2-crop_size//2:w//2+crop_size//2]\n",
    "\n",
    "    data = data.reshape(frame_num,c,crop_size,crop_size)\n",
    "    data = data - data.mean(dim=0, keepdim=True)\n",
    "    data = data[0]\n",
    "    data = data.reshape(c,crop_size,crop_size)\n",
    "    data = data[None]\n",
    "\n",
    "\n",
    "    if data_tensor_origin is None:\n",
    "        data_tensor_origin = data\n",
    "    else:\n",
    "        data_tensor_origin = torch.cat([data_tensor_origin, data],dim=0)\n",
    "\n",
    "\n",
    "data_tensor = data_tensor_origin.cuda()\n",
    "data_tensor = data_tensor.permute(0,2,3,1).reshape(total_data_num*crop_size*crop_size,c)\n",
    "\n",
    "# Center the data (subtract mean)\n",
    "mean = torch.mean(data_tensor, dim=0) # ~= 0\n",
    "centered_data = data_tensor - mean\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "covariance_matrix = torch.mm(centered_data.t(), centered_data) / (data_tensor.size(0) - 1)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors using torch.linalg.eig\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(covariance_matrix)\n",
    "eigenvalues = eigenvalues.to(torch.float)\n",
    "eigenvectors = eigenvectors.to(torch.float)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors\n",
    "sorted_indices = torch.argsort(eigenvalues[:], descending=True)\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Choose the top k principal components\n",
    "k = 2\n",
    "principal_components = eigenvectors[:, :k]\n",
    "projected_data = torch.mm(data_tensor, principal_components).detach().cpu()\n",
    "color = np.zeros([total_data_num*crop_size*crop_size*1])\n",
    "\n",
    "for i in range(total_data_num):\n",
    "    color[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1] = i/total_data_num\n",
    "\n",
    "labels = [\"Down 1\", \"Down 2\", \"Up 1\", \"Up 2\", \"Right 1\", \"Right 2\", \"Left 1\", \"Left 2\", \"Still 1\", \"Still 2\"]\n",
    "\n",
    "handles = []\n",
    "for i in range(total_data_num): \n",
    "    handle = plt.scatter(projected_data[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1,0].numpy(), projected_data[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1,1].numpy(), label=labels[i], marker='x')\n",
    "    handles.append(handle)\n",
    "\n",
    "plt.legend(handles=handles, bbox_to_anchor=(1.05,1.0))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('output/pca_moft.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without Content Correlation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_list = [\n",
    "    \"output/inter_feat/direction_down_scene1.pt\",\n",
    "    \"output/inter_feat/direction_down_scene2.pt\",\n",
    "    \"output/inter_feat/direction_up_scene1.pt\",\n",
    "    \"output/inter_feat/direction_up_scene2.pt\",\n",
    "    \"output/inter_feat/direction_right_scene1.pt\",\n",
    "    \"output/inter_feat/direction_right_scene2.pt\",\n",
    "    \"output/inter_feat/direction_left_scene1.pt\",\n",
    "    \"output/inter_feat/direction_left_scene2.pt\",\n",
    "    \"output/inter_feat/direction_still_scene1.pt\",\n",
    "    \"output/inter_feat/direction_still_scene2.pt\",\n",
    "]\n",
    "\n",
    "total_data_num = len(data_list)\n",
    "\n",
    "frame_num = 16\n",
    "\n",
    "data_tensor_origin = None\n",
    "for i, path in enumerate(data_list):\n",
    "    data = torch.load(path)\n",
    "    data = F.interpolate(data, [64, 64], mode='bilinear').to(torch.float32)\n",
    "\n",
    "    b,c,h,w = data.shape\n",
    "    crop_size = 16\n",
    "    data = data[:,:,h//2-crop_size//2:h//2+crop_size//2,w//2-crop_size//2:w//2+crop_size//2]\n",
    "\n",
    "    data = data.reshape(frame_num,c,crop_size,crop_size)\n",
    "    # data = data - data.mean(dim=0, keepdim=True) # only modify this\n",
    "    data = data[0]\n",
    "    data = data.reshape(c,crop_size,crop_size)\n",
    "    data = data[None]\n",
    "\n",
    "\n",
    "    if data_tensor_origin is None:\n",
    "        data_tensor_origin = data\n",
    "    else:\n",
    "        data_tensor_origin = torch.cat([data_tensor_origin, data],dim=0)\n",
    "\n",
    "\n",
    "data_tensor = data_tensor_origin.cuda()\n",
    "data_tensor = data_tensor.permute(0,2,3,1).reshape(total_data_num*crop_size*crop_size,c)\n",
    "\n",
    "# Center the data (subtract mean)\n",
    "mean = torch.mean(data_tensor, dim=0) # ~= 0\n",
    "centered_data = data_tensor - mean\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "covariance_matrix = torch.mm(centered_data.t(), centered_data) / (data_tensor.size(0) - 1)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors using torch.linalg.eig\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(covariance_matrix)\n",
    "eigenvalues = eigenvalues.to(torch.float)\n",
    "eigenvectors = eigenvectors.to(torch.float)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors\n",
    "sorted_indices = torch.argsort(eigenvalues[:], descending=True)\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Choose the top k principal components\n",
    "k = 2\n",
    "principal_components = eigenvectors[:, :k]\n",
    "\n",
    "projected_data = torch.mm(data_tensor, principal_components).detach().cpu()\n",
    "color = np.zeros([total_data_num*crop_size*crop_size*1])\n",
    "\n",
    "for i in range(total_data_num):\n",
    "    color[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1] = i/total_data_num\n",
    "\n",
    "labels = [\"Down 1\", \"Down 2\", \"Up 1\", \"Up 2\", \"Right 1\", \"Right 2\", \"Left 1\", \"Left 2\", \"Still 1\", \"Still 2\"]\n",
    "\n",
    "handles = []\n",
    "for i in range(total_data_num): \n",
    "    handle = plt.scatter(projected_data[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1,0].numpy(), projected_data[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1,1].numpy(), label=labels[i], marker='x')\n",
    "    handles.append(handle)\n",
    "\n",
    "plt.legend(handles=handles, bbox_to_anchor=(1.05,1.0))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('output/pca_moft_wo_CCR.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Channel Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_list = [\n",
    "    \"output/inter_feat/direction_left_scene1.pt\",\n",
    "    \"output/inter_feat/direction_left_scene2.pt\",\n",
    "    \"output/inter_feat/direction_right_scene1.pt\",\n",
    "    \"output/inter_feat/direction_right_scene2.pt\"\n",
    "]\n",
    "\n",
    "total_data_num = len(data_list)\n",
    "\n",
    "frame_num = 16\n",
    "\n",
    "data_tensor_origin = None\n",
    "for i, path in enumerate(data_list):\n",
    "    data = torch.load(path)\n",
    "    data = F.interpolate(data, [64, 64], mode='bilinear').to(torch.float32)\n",
    "\n",
    "    b,c,h,w = data.shape\n",
    "    crop_size = 16\n",
    "    data = data[:,:,h//2-crop_size//2:h//2+crop_size//2,w//2-crop_size//2:w//2+crop_size//2]\n",
    "\n",
    "    data = data.reshape(frame_num,c,crop_size,crop_size)\n",
    "    data = data - data.mean(dim=0, keepdim=True)\n",
    "    data = data[0]\n",
    "    data = data.reshape(c,crop_size,crop_size)\n",
    "    data = data[None]\n",
    "\n",
    "\n",
    "    if data_tensor_origin is None:\n",
    "        data_tensor_origin = data\n",
    "    else:\n",
    "        data_tensor_origin = torch.cat([data_tensor_origin, data],dim=0)\n",
    "\n",
    "\n",
    "data_tensor = data_tensor_origin.cuda()\n",
    "data_tensor = data_tensor.permute(0,2,3,1).reshape(total_data_num*crop_size*crop_size,c)\n",
    "\n",
    "# Center the data (subtract mean)\n",
    "mean = torch.mean(data_tensor, dim=0) # ~= 0\n",
    "centered_data = data_tensor - mean\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "covariance_matrix = torch.mm(centered_data.t(), centered_data) / (data_tensor.size(0) - 1)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors using torch.linalg.eig\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(covariance_matrix)\n",
    "eigenvalues = eigenvalues.to(torch.float)\n",
    "eigenvectors = eigenvectors.to(torch.float)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors\n",
    "sorted_indices = torch.argsort(eigenvalues[:], descending=True)\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Choose the top k principal components\n",
    "k = 2\n",
    "principal_components = eigenvectors[:, :k]\n",
    "_,idx = torch.sort(principal_components[:,0].abs(), descending=True)\n",
    "\n",
    "print(idx[:50])\n",
    "torch.save(idx[:50], \"output/horizontal_dims.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get MOFT from real videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import do_inversion, load_pipeline\n",
    "import os\n",
    "\n",
    "video_path = [\"assets/videos/car.gif\",\n",
    "              \"assets/videos/goose.mp4\"]\n",
    "outpath = [\"output/inter_feat/car.pt\",\n",
    "           \"output/inter_feat/goose.pt\"]\n",
    "\n",
    "pipeline = load_pipeline()\n",
    "do_inversion(pipeline, video_path, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate similarity between MOFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import Demo, convert_mp4_to_png\n",
    "\n",
    "dim_mask = torch.load(\"output/horizontal_dims.pt\").cpu()\n",
    "ft_1 = torch.load('output/inter_feat/goose.pt', map_location='cpu')\n",
    "ft_1 = ft_1[:,dim_mask]\n",
    "ft_1 = ft_1 - ft_1.mean(dim=0, keepdim=True)\n",
    "ft_1 = ft_1[0:1]\n",
    "ft_1 = F.interpolate(ft_1, (64, 64), mode='bilinear').reshape(-1,64,64)\n",
    "\n",
    "ft_2 = torch.load('output/inter_feat/car.pt', map_location='cpu').float()\n",
    "ft_2 = ft_2[:,dim_mask]\n",
    "ft_2 = ft_2 - ft_2.mean(dim=0, keepdim=True)\n",
    "ft_2 = ft_2[0:1]\n",
    "ft_2 = F.interpolate(ft_2, (64, 64), mode='bilinear').reshape(-1,64,64)\n",
    "\n",
    "ft = torch.stack([ft_1,ft_2],dim=0)\n",
    "\n",
    "imglist = []\n",
    "\n",
    "img_size = 400\n",
    "ensemble_size = 8\n",
    "\n",
    "filelist = [[\"assets/videos/goose.mp4\",0], [\"assets/videos/car.gif\",0]]\n",
    "\n",
    "\n",
    "for filename in filelist:\n",
    "    imglist.append(convert_mp4_to_png(filename[0], filename[1]))\n",
    "\n",
    "demo = Demo(imglist, ft, img_size=400)\n",
    "demo.ft = demo.ft.float()\n",
    "demo.plot_img_pairs(fig_size=5, x=200, y=200, output_path='output/similarity.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the trend in motion channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import json\n",
    "import os\n",
    "\n",
    "width, height = 400, 400\n",
    "motion_directions = [\"assets/motion_direction/direction_rightleft.json\",\n",
    "                     \"assets/motion_direction/direction_rightleftright.json\"]\n",
    "scene_directions = [\"assets/images/scene1.jpg\",\n",
    "                     \"assets/images/scene2.jpg\"]\n",
    "output_video_path = \"output/videos\"\n",
    "\n",
    "for motion_direction in motion_directions:\n",
    "    with open(motion_direction) as f:\n",
    "        velocity_list = json.load(f)['direction']\n",
    "\n",
    "    for scene_direction in scene_directions:\n",
    "        x = 0\n",
    "        y = 0\n",
    "        color_numpy_array = []\n",
    "        for idx, velocity in enumerate(velocity_list):\n",
    "            output_path = os.path.join(output_video_path, motion_direction.split(\"/\")[-1].split(\".\")[0]+\"_\"+scene_direction.split(\"/\")[-1].split(\".\")[0]+\".gif\")\n",
    "            image = Image.open(scene_direction)\n",
    "            image_width, image_height = image.size\n",
    "            array = np.array(image)\n",
    "            x += velocity[0]\n",
    "            y += velocity[1]\n",
    "            frame = array[y+image_height//2:y+image_height//2+height,x+image_width//2:x+image_width//2+width]\n",
    "            color_numpy_array.append(frame)\n",
    "\n",
    "        modified_frames = [frame + np.random.randint(0, 2, frame.shape, dtype=np.uint8) for frame in color_numpy_array] # prevent saving optimization\n",
    "        imageio.mimsave(output_path, modified_frames, duration=0.2)\n",
    "\n",
    "# Get the MOFT\n",
    "\n",
    "from utils import do_inversion, load_pipeline\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "video_path = [\"output/videos/direction_rightleft_scene1.gif\",\n",
    "              \"output/videos/direction_rightleft_scene2.gif\",\n",
    "              \"output/videos/direction_rightleftright_scene1.gif\",\n",
    "              \"output/videos/direction_rightleftright_scene2.gif\",]\n",
    "outpath = [\"output/inter_feat/direction_rightleft_scene1.pt\",\n",
    "           \"output/inter_feat/direction_rightleft_scene2.pt\",\n",
    "           \"output/inter_feat/direction_rightleftright_scene1.pt\",\n",
    "           \"output/inter_feat/direction_rightleftright_scene2.pt\"]\n",
    "\n",
    "pipeline = load_pipeline()\n",
    "do_inversion(pipeline, video_path, outpath)\n",
    "\n",
    "feat_list = [\n",
    "    ['output/inter_feat/direction_right_scene1.pt',\n",
    "     'output/inter_feat/direction_right_scene2.pt'],\n",
    "    ['output/inter_feat/direction_left_scene1.pt',\n",
    "     'output/inter_feat/direction_left_scene2.pt'],\n",
    "    ['output/inter_feat/direction_rightleft_scene1.pt',\n",
    "     'output/inter_feat/direction_rightleft_scene2.pt'],\n",
    "    ['output/inter_feat/direction_rightleftright_scene1.pt',\n",
    "     'output/inter_feat/direction_rightleftright_scene2.pt'],\n",
    "]\n",
    "\n",
    "label = ['right', 'left', 'rightleft', 'rightleftright']\n",
    "\n",
    "\n",
    "palette = plt.get_cmap('Set1')\n",
    "def draw_line(name_of_alg,color_index,datas):\n",
    "    color=palette(color_index)\n",
    "    avg=np.mean(datas,axis=0)\n",
    "    std=np.std(datas,axis=0)\n",
    "    iters=list(range(16))\n",
    "    r1 = list(map(lambda x: x[0]-x[1], zip(avg, std)))#上方差\n",
    "    r2 = list(map(lambda x: x[0]+x[1], zip(avg, std)))#下方差\n",
    "    handle, = plt.plot(iters, avg, color=color,label=name_of_alg, linewidth=3.5)\n",
    "    plt.fill_between(iters, r1, r2, color=color, alpha=0.2)\n",
    "    return handle\n",
    "\n",
    "handles = []\n",
    "for idx, feat in enumerate(feat_list):\n",
    "\n",
    "    f_list = []\n",
    "    for f in feat:\n",
    "        x = np.zeros(16*1*1)\n",
    "        for i in range(16):\n",
    "            x[i*1*1:(i+1)*1*1] = i\n",
    "\n",
    "        print(f)\n",
    "        a = torch.load(f)\n",
    "        print(a.shape)\n",
    "\n",
    "        a1 = a\n",
    "        a1 = a1 - a1.mean(0)[None]\n",
    "        y = a1[:,1194,5:20,5:20].reshape(16,-1).permute(1,0).cpu().numpy()\n",
    "        f_list.append(y)\n",
    "    y = np.concatenate(f_list, axis=0)\n",
    "    handles.append(draw_line(label[idx], idx,y))\n",
    "\n",
    "legend = plt.legend(handles=handles)\n",
    "\n",
    "plt.xlabel('Frame Index', size = 20)\n",
    "plt.ylabel('Channel Value', size = 20)\n",
    "\n",
    "plt.xticks(size = 20)\n",
    "plt.yticks(size = 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "# # 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图形\n",
    "# plt.show()\n",
    "plt.savefig(\"output/channel_vis_1194.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MOFT for Video Motion Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an origin video\n",
    "from utils import load_pipeline, save_videos_grid\n",
    "import torch\n",
    "\n",
    "pipeline = load_pipeline()\n",
    "\n",
    "prompt = \"close up photo of two cats sitting in a room\"\n",
    "negative_prompt = \"semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate\"\n",
    "latents = torch.load(\"assets/double_cats.pt\")\n",
    "sample = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=7.5,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    video_length=16,\n",
    "    latents=latents,\n",
    "    motion_guide=False\n",
    ").videos\n",
    "\n",
    "save_videos_grid(sample, \"output/origin.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion control - single point\n",
    "\n",
    "sample = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=7.5,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    video_length=16,\n",
    "    motion_guide=True,\n",
    "    latents=latents,\n",
    "    motion_guidance_weight=8,\n",
    "    mask_path=[\"assets/masks/cat_left.png\"],\n",
    "    dim_mask_path=['output/horizontal_dims.pt'],\n",
    "    reference_moft_path=['output/inter_feat/direction_rightleft_scene1.pt']\n",
    ").videos\n",
    "\n",
    "save_videos_grid(sample, \"output/single_point.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion control - multiple points\n",
    "\n",
    "sample = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=7.5,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    video_length=16,\n",
    "    motion_guide=True,\n",
    "    latents=latents,\n",
    "    motion_guidance_weight=8,\n",
    "    mask_path=[\"assets/masks/cat_left.png\", \"assets/masks/cat_right.png\"],\n",
    "    dim_mask_path=['output/horizontal_dims.pt','output/horizontal_dims.pt'],\n",
    "    reference_moft_path=['output/inter_feat/direction_rightleft_scene1.pt', 'output/inter_feat/direction_rightleft_scene1.pt']\n",
    ").videos\n",
    "\n",
    "save_videos_grid(sample, \"output/multi_points.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animatediff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
