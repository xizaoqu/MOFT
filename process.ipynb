{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate videos with different moving directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import json\n",
    "import os\n",
    "\n",
    "width, height = 400, 400\n",
    "motion_directions = [\"assets/motion_direction/direction_left.json\",\n",
    "                     \"assets/motion_direction/direction_right.json\",\n",
    "                     \"assets/motion_direction/direction_up.json\",\n",
    "                     \"assets/motion_direction/direction_down.json\",\n",
    "                     \"assets/motion_direction/direction_still.json\"]\n",
    "scene_directions = [\"assets/images/scene1.jpg\",\n",
    "                     \"assets/images/scene2.jpg\"]\n",
    "output_video_path = \"output/videos\"\n",
    "os.makedirs(output_video_path, exist_ok=True)\n",
    "\n",
    "for motion_direction in motion_directions:\n",
    "    with open(motion_direction) as f:\n",
    "        velocity_list = json.load(f)['direction']\n",
    "\n",
    "    for scene_direction in scene_directions:\n",
    "        x = 0\n",
    "        y = 0\n",
    "        color_numpy_array = []\n",
    "        for idx, velocity in enumerate(velocity_list):\n",
    "            output_path = os.path.join(output_video_path, motion_direction.split(\"/\")[-1].split(\".\")[0]+\"_\"+scene_direction.split(\"/\")[-1].split(\".\")[0]+\".gif\")\n",
    "            image = Image.open(scene_direction)\n",
    "            image_width, image_height = image.size\n",
    "            array = np.array(image)\n",
    "            x += velocity[0]\n",
    "            y += velocity[1]\n",
    "            frame = array[y+image_height//2:y+image_height//2+height,x+image_width//2:x+image_width//2+width]\n",
    "            color_numpy_array.append(frame)\n",
    "\n",
    "        modified_frames = [frame + np.random.randint(0, 2, frame.shape, dtype=np.uint8) for frame in color_numpy_array] # prevent saving optimization\n",
    "        imageio.mimsave(output_path, modified_frames, duration=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get intermediate feature from DDIM inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/slurm_home/zqxiao/miniconda3/envs/animatediff/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded temporal unet's pretrained weights from models/StableDiffusion/unet ...\n",
      "### missing keys: 588; \n",
      "### unexpected keys: 0;\n",
      "### Temporal Module Parameters: 453.20928 M\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "torch.cuda.is_available() should be True but is False. xformers' memory efficient attention is only available for GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m outpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_features/direction_down_scene1.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_features\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdo_inversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MOFT/inversion.py:62\u001b[0m, in \u001b[0;36mdo_inversion\u001b[0;34m(video_path, outpath)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_inversion\u001b[39m(video_path, outpath):\n\u001b[0;32m---> 62\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mload_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39msave_inter_feat(video_path\u001b[38;5;241m=\u001b[39mvideo_path,\n\u001b[1;32m     64\u001b[0m                                 prompts\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, outpath\u001b[38;5;241m=\u001b[39moutpath)\n",
      "File \u001b[0;32m~/MOFT/inversion.py:36\u001b[0m, in \u001b[0;36mload_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m unet \u001b[38;5;241m=\u001b[39m UNet3DConditionModel\u001b[38;5;241m.\u001b[39mfrom_pretrained_2d(\n\u001b[1;32m     33\u001b[0m     pretrained_model_path, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munet\u001b[39m\u001b[38;5;124m\"\u001b[39m, unet_additional_kwargs\u001b[38;5;241m=\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mto_container(inference_config\u001b[38;5;241m.\u001b[39munet_additional_kwargs))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_xformers_available():\n\u001b[0;32m---> 36\u001b[0m     \u001b[43munet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_xformers_memory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/modeling_utils.py:215\u001b[0m, in \u001b[0;36mModelMixin.enable_xformers_memory_efficient_attention\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menable_xformers_memory_efficient_attention\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    Enable memory efficient attention as implemented in xformers.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    is used.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_memory_efficient_attention_xformers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/modeling_utils.py:203\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers\u001b[0;34m(self, valid)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m--> 203\u001b[0m         \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/modeling_utils.py:199\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    196\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/modeling_utils.py:199\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    196\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/modeling_utils.py:199\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    196\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/modeling_utils.py:196\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_recursive_set_mem_eff\u001b[39m(module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_use_memory_efficient_attention_xformers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 196\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_memory_efficient_attention_xformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    199\u001b[0m         fn_recursive_set_mem_eff(child)\n",
      "File \u001b[0;32m~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/modeling_utils.py:203\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers\u001b[0;34m(self, valid)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m--> 203\u001b[0m         \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/modeling_utils.py:199\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    196\u001b[0m     module\u001b[38;5;241m.\u001b[39mset_use_memory_efficient_attention_xformers(valid)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 199\u001b[0m     \u001b[43mfn_recursive_set_mem_eff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/animatediff/lib/python3.10/site-packages/diffusers/modeling_utils.py:196\u001b[0m, in \u001b[0;36mModelMixin.set_use_memory_efficient_attention_xformers.<locals>.fn_recursive_set_mem_eff\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_recursive_set_mem_eff\u001b[39m(module: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_use_memory_efficient_attention_xformers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 196\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_use_memory_efficient_attention_xformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m    199\u001b[0m         fn_recursive_set_mem_eff(child)\n",
      "File \u001b[0;32m~/MOFT/animatediff/models/my_attention.py:353\u001b[0m, in \u001b[0;36mBasicTransformerBlock.set_use_memory_efficient_attention_xformers\u001b[0;34m(self, use_memory_efficient_attention_xformers)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRefer to https://github.com/facebookresearch/xformers for more information on how to install\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m xformers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    350\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxformers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.cuda.is_available() should be True but is False. xformers\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m memory efficient attention is only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m available for GPU \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    356\u001b[0m     )\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;66;03m# Make sure we can run the memory efficient attention\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: torch.cuda.is_available() should be True but is False. xformers' memory efficient attention is only available for GPU "
     ]
    }
   ],
   "source": [
    "from utils import do_inversion, load_pipeline\n",
    "import os\n",
    "\n",
    "video_path = [\"output/videos/direction_down_scene1.gif\",\n",
    "              \"output/videos/direction_down_scene2.gif\",\n",
    "              \"output/videos/direction_up_scene1.gif\",\n",
    "              \"output/videos/direction_up_scene2.gif\",\n",
    "              \"output/videos/direction_right_scene1.gif\",\n",
    "              \"output/videos/direction_right_scene2.gif\",\n",
    "              \"output/videos/direction_left_scene1.gif\",\n",
    "              \"output/videos/direction_left_scene2.gif\",\n",
    "              \"output/videos/direction_still_scene1.gif\",\n",
    "              \"output/videos/direction_still_scene2.gif\",]\n",
    "outpath = [\"output/inter_feat/direction_down_scene1.pt\",\n",
    "           \"output/inter_feat/direction_down_scene2.pt\",\n",
    "           \"output/inter_feat/direction_up_scene1.pt\",\n",
    "           \"output/inter_feat/direction_up_scene2.pt\",\n",
    "           \"output/inter_feat/direction_right_scene1.pt\",\n",
    "           \"output/inter_feat/direction_right_scene2.pt\",\n",
    "           \"output/inter_feat/direction_left_scene1.pt\",\n",
    "           \"output/inter_feat/direction_left_scene2.pt\",\n",
    "           \"output/inter_feat/direction_still_scene1.pt\",\n",
    "           \"output/inter_feat/direction_still_scene2.pt\",]\n",
    "\n",
    "os.makedirs('output/inter_feat', exist_ok=True)\n",
    "\n",
    "pipeline = load_pipeline()\n",
    "do_inversion(pipeline, video_path, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Content Correlation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_list = [\n",
    "    \"output/inter_feat/direction_down_scene1.pt\",\n",
    "    \"output/inter_feat/direction_down_scene2.pt\",\n",
    "    \"output/inter_feat/direction_up_scene1.pt\",\n",
    "    \"output/inter_feat/direction_up_scene2.pt\",\n",
    "    \"output/inter_feat/direction_right_scene1.pt\",\n",
    "    \"output/inter_feat/direction_right_scene2.pt\",\n",
    "    \"output/inter_feat/direction_left_scene1.pt\",\n",
    "    \"output/inter_feat/direction_left_scene2.pt\",\n",
    "    \"output/inter_feat/direction_still_scene1.pt\",\n",
    "    \"output/inter_feat/direction_still_scene2.pt\",\n",
    "]\n",
    "\n",
    "total_data_num = len(data_list)\n",
    "\n",
    "frame_num = 16\n",
    "\n",
    "data_tensor_origin = None\n",
    "for i, path in enumerate(data_list):\n",
    "    data = torch.load(path)\n",
    "    data = F.interpolate(data, [64, 64], mode='bilinear').to(torch.float32)\n",
    "\n",
    "    b,c,h,w = data.shape\n",
    "    crop_size = 16\n",
    "    data = data[:,:,h//2-crop_size//2:h//2+crop_size//2,w//2-crop_size//2:w//2+crop_size//2]\n",
    "\n",
    "    data = data.reshape(frame_num,c,crop_size,crop_size)\n",
    "    data = data - data.mean(dim=0, keepdim=True)\n",
    "    data = data[0]\n",
    "    data = data.reshape(c,crop_size,crop_size)\n",
    "    data = data[None]\n",
    "\n",
    "\n",
    "    if data_tensor_origin is None:\n",
    "        data_tensor_origin = data\n",
    "    else:\n",
    "        data_tensor_origin = torch.cat([data_tensor_origin, data],dim=0)\n",
    "\n",
    "\n",
    "data_tensor = data_tensor_origin.cuda()\n",
    "data_tensor = data_tensor.permute(0,2,3,1).reshape(total_data_num*crop_size*crop_size,c)\n",
    "\n",
    "# Center the data (subtract mean)\n",
    "mean = torch.mean(data_tensor, dim=0) # ~= 0\n",
    "centered_data = data_tensor - mean\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "covariance_matrix = torch.mm(centered_data.t(), centered_data) / (data_tensor.size(0) - 1)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors using torch.linalg.eig\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(covariance_matrix)\n",
    "eigenvalues = eigenvalues.to(torch.float)\n",
    "eigenvectors = eigenvectors.to(torch.float)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors\n",
    "sorted_indices = torch.argsort(eigenvalues[:], descending=True)\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Choose the top k principal components\n",
    "k = 2\n",
    "principal_components = eigenvectors[:, :k]\n",
    "projected_data = torch.mm(data_tensor, principal_components).detach().cpu()\n",
    "color = np.zeros([total_data_num*crop_size*crop_size*1])\n",
    "\n",
    "for i in range(total_data_num):\n",
    "    color[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1] = i/total_data_num\n",
    "\n",
    "labels = [\"Down 1\", \"Down 2\", \"Up 1\", \"Up 2\", \"Right 1\", \"Right 2\", \"Left 1\", \"Left 2\", \"Still 1\", \"Still 2\"]\n",
    "\n",
    "handles = []\n",
    "for i in range(total_data_num): \n",
    "    handle = plt.scatter(projected_data[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1,0].numpy(), projected_data[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1,1].numpy(), label=labels[i], marker='x')\n",
    "    handles.append(handle)\n",
    "\n",
    "plt.legend(handles=handles, bbox_to_anchor=(1.05,1.0))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('output/pca_moft.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without Content Correlation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_list = [\n",
    "    \"output/inter_feat/direction_down_scene1.pt\",\n",
    "    \"output/inter_feat/direction_down_scene2.pt\",\n",
    "    \"output/inter_feat/direction_up_scene1.pt\",\n",
    "    \"output/inter_feat/direction_up_scene2.pt\",\n",
    "    \"output/inter_feat/direction_right_scene1.pt\",\n",
    "    \"output/inter_feat/direction_right_scene2.pt\",\n",
    "    \"output/inter_feat/direction_left_scene1.pt\",\n",
    "    \"output/inter_feat/direction_left_scene2.pt\",\n",
    "    \"output/inter_feat/direction_still_scene1.pt\",\n",
    "    \"output/inter_feat/direction_still_scene2.pt\",\n",
    "]\n",
    "\n",
    "total_data_num = len(data_list)\n",
    "\n",
    "frame_num = 16\n",
    "\n",
    "data_tensor_origin = None\n",
    "for i, path in enumerate(data_list):\n",
    "    data = torch.load(path)\n",
    "    data = F.interpolate(data, [64, 64], mode='bilinear').to(torch.float32)\n",
    "\n",
    "    b,c,h,w = data.shape\n",
    "    crop_size = 16\n",
    "    data = data[:,:,h//2-crop_size//2:h//2+crop_size//2,w//2-crop_size//2:w//2+crop_size//2]\n",
    "\n",
    "    data = data.reshape(frame_num,c,crop_size,crop_size)\n",
    "    # data = data - data.mean(dim=0, keepdim=True) # only modify this\n",
    "    data = data[0]\n",
    "    data = data.reshape(c,crop_size,crop_size)\n",
    "    data = data[None]\n",
    "\n",
    "\n",
    "    if data_tensor_origin is None:\n",
    "        data_tensor_origin = data\n",
    "    else:\n",
    "        data_tensor_origin = torch.cat([data_tensor_origin, data],dim=0)\n",
    "\n",
    "\n",
    "data_tensor = data_tensor_origin.cuda()\n",
    "data_tensor = data_tensor.permute(0,2,3,1).reshape(total_data_num*crop_size*crop_size,c)\n",
    "\n",
    "# Center the data (subtract mean)\n",
    "mean = torch.mean(data_tensor, dim=0) # ~= 0\n",
    "centered_data = data_tensor - mean\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "covariance_matrix = torch.mm(centered_data.t(), centered_data) / (data_tensor.size(0) - 1)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors using torch.linalg.eig\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(covariance_matrix)\n",
    "eigenvalues = eigenvalues.to(torch.float)\n",
    "eigenvectors = eigenvectors.to(torch.float)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors\n",
    "sorted_indices = torch.argsort(eigenvalues[:], descending=True)\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Choose the top k principal components\n",
    "k = 2\n",
    "principal_components = eigenvectors[:, :k]\n",
    "\n",
    "projected_data = torch.mm(data_tensor, principal_components).detach().cpu()\n",
    "color = np.zeros([total_data_num*crop_size*crop_size*1])\n",
    "\n",
    "for i in range(total_data_num):\n",
    "    color[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1] = i/total_data_num\n",
    "\n",
    "labels = [\"Down 1\", \"Down 2\", \"Up 1\", \"Up 2\", \"Right 1\", \"Right 2\", \"Left 1\", \"Left 2\", \"Still 1\", \"Still 2\"]\n",
    "\n",
    "handles = []\n",
    "for i in range(total_data_num): \n",
    "    handle = plt.scatter(projected_data[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1,0].numpy(), projected_data[i*crop_size*crop_size*1:(i+1)*crop_size*crop_size*1,1].numpy(), label=labels[i], marker='x')\n",
    "    handles.append(handle)\n",
    "\n",
    "plt.legend(handles=handles, bbox_to_anchor=(1.05,1.0))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('output/pca_moft_wo_CCR.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Channel Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_list = [\n",
    "    \"output/inter_feat/direction_left_scene1.pt\",\n",
    "    \"output/inter_feat/direction_left_scene2.pt\",\n",
    "    \"output/inter_feat/direction_right_scene1.pt\",\n",
    "    \"output/inter_feat/direction_right_scene2.pt\"\n",
    "]\n",
    "\n",
    "total_data_num = len(data_list)\n",
    "\n",
    "frame_num = 16\n",
    "\n",
    "data_tensor_origin = None\n",
    "for i, path in enumerate(data_list):\n",
    "    data = torch.load(path)\n",
    "    data = F.interpolate(data, [64, 64], mode='bilinear').to(torch.float32)\n",
    "\n",
    "    b,c,h,w = data.shape\n",
    "    crop_size = 16\n",
    "    data = data[:,:,h//2-crop_size//2:h//2+crop_size//2,w//2-crop_size//2:w//2+crop_size//2]\n",
    "\n",
    "    data = data.reshape(frame_num,c,crop_size,crop_size)\n",
    "    data = data - data.mean(dim=0, keepdim=True)\n",
    "    data = data[0]\n",
    "    data = data.reshape(c,crop_size,crop_size)\n",
    "    data = data[None]\n",
    "\n",
    "\n",
    "    if data_tensor_origin is None:\n",
    "        data_tensor_origin = data\n",
    "    else:\n",
    "        data_tensor_origin = torch.cat([data_tensor_origin, data],dim=0)\n",
    "\n",
    "\n",
    "data_tensor = data_tensor_origin.cuda()\n",
    "data_tensor = data_tensor.permute(0,2,3,1).reshape(total_data_num*crop_size*crop_size,c)\n",
    "\n",
    "# Center the data (subtract mean)\n",
    "mean = torch.mean(data_tensor, dim=0) # ~= 0\n",
    "centered_data = data_tensor - mean\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "covariance_matrix = torch.mm(centered_data.t(), centered_data) / (data_tensor.size(0) - 1)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors using torch.linalg.eig\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(covariance_matrix)\n",
    "eigenvalues = eigenvalues.to(torch.float)\n",
    "eigenvectors = eigenvectors.to(torch.float)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors\n",
    "sorted_indices = torch.argsort(eigenvalues[:], descending=True)\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Choose the top k principal components\n",
    "k = 2\n",
    "principal_components = eigenvectors[:, :k]\n",
    "_,idx = torch.sort(principal_components[:,0].abs(), descending=True)\n",
    "\n",
    "print(idx[:50])\n",
    "torch.save(idx[:50], \"output/horizontal_dims.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get MOFT from real videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import do_inversion, load_pipeline\n",
    "import os\n",
    "\n",
    "video_path = [\"assets/videos/car.gif\",\n",
    "              \"assets/videos/goose.mp4\"]\n",
    "outpath = [\"output/inter_feat/car.pt\",\n",
    "           \"output/inter_feat/goose.pt\"]\n",
    "\n",
    "pipeline = load_pipeline()\n",
    "do_inversion(pipeline, video_path, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate similarity between MOFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils import Demo, convert_mp4_to_png\n",
    "\n",
    "dim_mask = torch.load(\"output/horizontal_dims.pt\").cpu()\n",
    "ft_1 = torch.load('output/inter_feat/goose.pt', map_location='cpu')\n",
    "ft_1 = ft_1[:,dim_mask]\n",
    "ft_1 = ft_1 - ft_1.mean(dim=0, keepdim=True)\n",
    "ft_1 = ft_1[0:1]\n",
    "ft_1 = F.interpolate(ft_1, (64, 64), mode='bilinear').reshape(-1,64,64)\n",
    "\n",
    "ft_2 = torch.load('output/inter_feat/car.pt', map_location='cpu').float()\n",
    "ft_2 = ft_2[:,dim_mask]\n",
    "ft_2 = ft_2 - ft_2.mean(dim=0, keepdim=True)\n",
    "ft_2 = ft_2[0:1]\n",
    "ft_2 = F.interpolate(ft_2, (64, 64), mode='bilinear').reshape(-1,64,64)\n",
    "\n",
    "ft = torch.stack([ft_1,ft_2],dim=0)\n",
    "\n",
    "imglist = []\n",
    "\n",
    "img_size = 400\n",
    "ensemble_size = 8\n",
    "\n",
    "filelist = [[\"assets/videos/goose.mp4\",0], [\"assets/videos/car.gif\",0]]\n",
    "\n",
    "\n",
    "for filename in filelist:\n",
    "    imglist.append(convert_mp4_to_png(filename[0], filename[1]))\n",
    "\n",
    "demo = Demo(imglist, ft, img_size=400)\n",
    "demo.ft = demo.ft.float()\n",
    "demo.plot_img_pairs(fig_size=5, x=200, y=200, output_path='output/similarity.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the trend in motion channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import json\n",
    "import os\n",
    "\n",
    "width, height = 400, 400\n",
    "motion_directions = [\"assets/motion_direction/direction_rightleft.json\",\n",
    "                     \"assets/motion_direction/direction_rightleftright.json\"]\n",
    "scene_directions = [\"assets/images/scene1.jpg\",\n",
    "                     \"assets/images/scene2.jpg\"]\n",
    "output_video_path = \"output/videos\"\n",
    "\n",
    "for motion_direction in motion_directions:\n",
    "    with open(motion_direction) as f:\n",
    "        velocity_list = json.load(f)['direction']\n",
    "\n",
    "    for scene_direction in scene_directions:\n",
    "        x = 0\n",
    "        y = 0\n",
    "        color_numpy_array = []\n",
    "        for idx, velocity in enumerate(velocity_list):\n",
    "            output_path = os.path.join(output_video_path, motion_direction.split(\"/\")[-1].split(\".\")[0]+\"_\"+scene_direction.split(\"/\")[-1].split(\".\")[0]+\".gif\")\n",
    "            image = Image.open(scene_direction)\n",
    "            image_width, image_height = image.size\n",
    "            array = np.array(image)\n",
    "            x += velocity[0]\n",
    "            y += velocity[1]\n",
    "            frame = array[y+image_height//2:y+image_height//2+height,x+image_width//2:x+image_width//2+width]\n",
    "            color_numpy_array.append(frame)\n",
    "\n",
    "        modified_frames = [frame + np.random.randint(0, 2, frame.shape, dtype=np.uint8) for frame in color_numpy_array] # prevent saving optimization\n",
    "        imageio.mimsave(output_path, modified_frames, duration=0.2)\n",
    "\n",
    "# Get the MOFT\n",
    "\n",
    "from utils import do_inversion, load_pipeline\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "video_path = [\"output/videos/direction_rightleft_scene1.gif\",\n",
    "              \"output/videos/direction_rightleft_scene2.gif\",\n",
    "              \"output/videos/direction_rightleftright_scene1.gif\",\n",
    "              \"output/videos/direction_rightleftright_scene2.gif\",]\n",
    "outpath = [\"output/inter_feat/direction_rightleft_scene1.pt\",\n",
    "           \"output/inter_feat/direction_rightleft_scene2.pt\",\n",
    "           \"output/inter_feat/direction_rightleftright_scene1.pt\",\n",
    "           \"output/inter_feat/direction_rightleftright_scene2.pt\"]\n",
    "\n",
    "pipeline = load_pipeline()\n",
    "do_inversion(pipeline, video_path, outpath)\n",
    "\n",
    "feat_list = [\n",
    "    ['output/inter_feat/direction_right_scene1.pt',\n",
    "     'output/inter_feat/direction_right_scene2.pt'],\n",
    "    ['output/inter_feat/direction_left_scene1.pt',\n",
    "     'output/inter_feat/direction_left_scene2.pt'],\n",
    "    ['output/inter_feat/direction_rightleft_scene1.pt',\n",
    "     'output/inter_feat/direction_rightleft_scene2.pt'],\n",
    "    ['output/inter_feat/direction_rightleftright_scene1.pt',\n",
    "     'output/inter_feat/direction_rightleftright_scene2.pt'],\n",
    "]\n",
    "\n",
    "label = ['right', 'left', 'rightleft', 'rightleftright']\n",
    "\n",
    "\n",
    "palette = plt.get_cmap('Set1')\n",
    "def draw_line(name_of_alg,color_index,datas):\n",
    "    color=palette(color_index)\n",
    "    avg=np.mean(datas,axis=0)\n",
    "    std=np.std(datas,axis=0)\n",
    "    iters=list(range(16))\n",
    "    r1 = list(map(lambda x: x[0]-x[1], zip(avg, std)))#上方差\n",
    "    r2 = list(map(lambda x: x[0]+x[1], zip(avg, std)))#下方差\n",
    "    handle, = plt.plot(iters, avg, color=color,label=name_of_alg, linewidth=3.5)\n",
    "    plt.fill_between(iters, r1, r2, color=color, alpha=0.2)\n",
    "    return handle\n",
    "\n",
    "handles = []\n",
    "for idx, feat in enumerate(feat_list):\n",
    "\n",
    "    f_list = []\n",
    "    for f in feat:\n",
    "        x = np.zeros(16*1*1)\n",
    "        for i in range(16):\n",
    "            x[i*1*1:(i+1)*1*1] = i\n",
    "\n",
    "        print(f)\n",
    "        a = torch.load(f)\n",
    "        print(a.shape)\n",
    "\n",
    "        a1 = a\n",
    "        a1 = a1 - a1.mean(0)[None]\n",
    "        y = a1[:,1194,5:20,5:20].reshape(16,-1).permute(1,0).cpu().numpy()\n",
    "        f_list.append(y)\n",
    "    y = np.concatenate(f_list, axis=0)\n",
    "    handles.append(draw_line(label[idx], idx,y))\n",
    "\n",
    "legend = plt.legend(handles=handles)\n",
    "\n",
    "plt.xlabel('Frame Index', size = 20)\n",
    "plt.ylabel('Channel Value', size = 20)\n",
    "\n",
    "plt.xticks(size = 20)\n",
    "plt.yticks(size = 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "# # 添加图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图形\n",
    "# plt.show()\n",
    "plt.savefig(\"output/channel_vis_1194.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use MOFT for Video Motion Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an origin video\n",
    "from utils import load_pipeline, save_videos_grid\n",
    "import torch\n",
    "\n",
    "pipeline = load_pipeline()\n",
    "\n",
    "prompt = \"close up photo of two cats sitting in a room\"\n",
    "negative_prompt = \"semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate\"\n",
    "latents = torch.load(\"assets/double_cats.pt\")\n",
    "sample = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=7.5,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    video_length=16,\n",
    "    latents=latents,\n",
    "    motion_guide=False\n",
    ").videos\n",
    "\n",
    "save_videos_grid(sample, \"output/origin.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion control - single point\n",
    "\n",
    "sample = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=7.5,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    video_length=16,\n",
    "    motion_guide=True,\n",
    "    latents=latents,\n",
    "    motion_guidance_weight=8,\n",
    "    mask_path=[\"assets/masks/cat_left.png\"],\n",
    "    dim_mask_path=['output/horizontal_dims.pt'],\n",
    "    reference_moft_path=['output/inter_feat/direction_rightleft_scene1.pt']\n",
    ").videos\n",
    "\n",
    "save_videos_grid(sample, \"output/single_point.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motion control - multiple points\n",
    "\n",
    "sample = pipeline(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=25,\n",
    "    guidance_scale=7.5,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    video_length=16,\n",
    "    motion_guide=True,\n",
    "    latents=latents,\n",
    "    motion_guidance_weight=8,\n",
    "    mask_path=[\"assets/masks/cat_left.png\", \"assets/masks/cat_right.png\"],\n",
    "    dim_mask_path=['output/horizontal_dims.pt','output/horizontal_dims.pt'],\n",
    "    reference_moft_path=['output/inter_feat/direction_rightleft_scene1.pt', 'output/inter_feat/direction_rightleft_scene1.pt']\n",
    ").videos\n",
    "\n",
    "save_videos_grid(sample, \"output/multi_points.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "animatediff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
